{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bez poprawki LaPlace\n",
      "dokladnosc na zbiorze uczacym:  0.9831932773109243\n",
      "dokladnosc na zbiorze testowym:  0.9152542372881356\n",
      "z poprawka LaPlace\n",
      "dokladnosc na zbiorze uczacym:  0.9663865546218487\n",
      "dokladnosc na zbiorze testowym:  0.9322033898305084\n",
      "dla malych zbiorow(tutaj zbioru uczacego) poprawka LaPlace nieznacznie psuje dokladnosc,\n",
      "ale dla danych testowych zwieksza ja \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cw 6.1\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt(\"wine.txt\",dtype=float, delimiter=',')\n",
    "X = data[:,1:len(data[0])]\n",
    "y = data[:,0]\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "bins = 4 #poziomow bo jak nie bd w zbiorze wartosci danej to nie bd zwracac dla tego prawdopodobienstwa warunkowego\n",
    "est = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='uniform')\n",
    "est.fit(X)\n",
    "Xt = est.transform(X)   #zbior uczacy zdyskretyzowany \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.33, random_state=41)   #zbior testowy\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "                        \n",
    "class BayesClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y, bins, laPlace = False): #przekazuje fit liczbe przedzialow wartosci\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bins = bins\n",
    "        self.laPlace = laPlace\n",
    "        self.iloscKlas = len(np.unique(y))\n",
    "        self.klasy = np.unique(y)\n",
    "        self.iloscAtrybutow = self.X.shape[1]\n",
    "        #obliczenie a priori\n",
    "        self.aPriori = np.array([])\n",
    "        for klasa in np.unique(self.y):\n",
    "            classProb = len(self.y[self.y == klasa])/len(self.y)\n",
    "            self.aPriori = np.append(self.aPriori, classProb)\n",
    "        #obliczenie rozk warunkowego\n",
    "        self.rozkWarunkowy = np.zeros((self.iloscKlas, self.iloscAtrybutow, self.bins))\n",
    "        itKlasy = 0 \n",
    "        for klasa in np.unique(self.y): # w klasie\n",
    "            itAtryb = 0\n",
    "            for atryb in self.X[self.y==klasa].T: #dla kazdego atrybutu inne prawdopodobienstwo warunkowe   # w kolumnie\n",
    "                for wartosc in range(self.bins): \n",
    "                    if(self.laPlace):\n",
    "                        pWar = (len(atryb[atryb == wartosc]) + 1) / (len(atryb) + self.bins)\n",
    "                    else:\n",
    "                        pWar = len(atryb[atryb == wartosc])/len(atryb)\n",
    "                    self.rozkWarunkowy[itKlasy][itAtryb][wartosc] = pWar\n",
    "                itAtryb += 1\n",
    "            itKlasy += 1\n",
    "            \n",
    "        #print(self.rozkWarunkowy.shape)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        iloscProbek = X.shape[0]\n",
    "        aPosteriori = np.ones((iloscProbek, self.iloscKlas))\n",
    "        probkaIdx = 0\n",
    "        for probka in X:\n",
    "            for klasaIdx in range(self.iloscKlas):\n",
    "                for atrybutIdx in range(self.iloscAtrybutow):\n",
    "                    aPosteriori[probkaIdx][klasaIdx] *= self.rozkWarunkowy[klasaIdx][atrybutIdx][int(probka[atrybutIdx])]\n",
    "                aPosteriori[probkaIdx][klasaIdx] *= self.aPriori[klasaIdx]\n",
    "            probkaIdx += 1\n",
    "        return self.klasy[np.argmax(aPosteriori, axis=1)]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        iloscProbek = X.shape[0]\n",
    "        aPosteriori = np.ones((iloscProbek, self.iloscKlas))\n",
    "        probkaIdx = 0\n",
    "        for probka in X:\n",
    "            for klasaIdx in range(self.iloscKlas):\n",
    "                for atrybutIdx in range(self.iloscAtrybutow):\n",
    "                    aPosteriori[probkaIdx][klasaIdx] *= self.rozkWarunkowy[klasaIdx][atrybutIdx][int(probka[atrybutIdx])]\n",
    "                aPosteriori[probkaIdx][klasaIdx] *= self.aPriori[klasaIdx]\n",
    "            probkaIdx += 1\n",
    "        \n",
    "        prob = np.zeros((aPosteriori.shape[0],aPosteriori.shape[1]))\n",
    "        for i in range(len(prob)):\n",
    "            for j in range(len(prob[0])):\n",
    "                if(not self.laPlace and (np.sum(aPosteriori[i]) == 0)):   #gdy iloczyn = 0 bo brak poprawki LapPlace, zmieniam prawodpodobienstwa klas na takie same\n",
    "                    prob[i] = 1/self.iloscKlas\n",
    "                    break\n",
    "                else:\n",
    "                    prob[i][j]  = (aPosteriori[i][j]) /np.sum(aPosteriori[i])\n",
    "        return prob\n",
    "\n",
    "bc= BayesClassifier()\n",
    "\n",
    "bc.fit(X_train, y_train, bins, False)\n",
    "print(\"bez poprawki LaPlace\")\n",
    "print(\"dokladnosc na zbiorze uczacym: \", bc.score(X_train, y_train))\n",
    "print(\"dokladnosc na zbiorze testowym: \", bc.score(X_test, y_test))\n",
    "print(\"z poprawka LaPlace\")\n",
    "bc.fit(X_train, y_train, bins, True)\n",
    "print(\"dokladnosc na zbiorze uczacym: \", bc.score(X_train, y_train))\n",
    "print(\"dokladnosc na zbiorze testowym: \", bc.score(X_test, y_test))\n",
    "print(\"dla malych zbiorow(tutaj zbioru uczacego) poprawka LaPlace nieznacznie psuje dokladnosc,\\n\\\n",
    "ale dla danych testowych zwieksza ja \\n\")  \n",
    "\n",
    "#prob = bc.predict_proba(X_test)\n",
    "#np.amax(prob, axis=1)\n",
    "#print(bc.get_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wariant ciagly\n",
      "dokladnosc na zbiorze testowym:  0.9491525423728814\n",
      "wariant ciagly - gotowa implementacja\n",
      "dokladnosc na zbiorze testowym:  0.9491525423728814\n",
      "wariant dyskretny bez poprawki LaPlace\n",
      "dokladnosc na zbiorze testowym:  0.9152542372881356\n",
      "wariant dyskretny z poprawka LaPlace\n",
      "dokladnosc na zbiorze testowym:  0.9322033898305084\n",
      "dokladnosc klasyfikatora Bayesa na danych ciaglych jest wieksza\n"
     ]
    }
   ],
   "source": [
    "#Cw 6.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=41)   #zbior testowy\n",
    "\n",
    "class BayesClassifierContinuousData(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y): #przekazuje fit liczbe przedzialow wartosci\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.iloscKlas = len(np.unique(y))\n",
    "        self.klasy = np.unique(y)\n",
    "        self.iloscAtrybutow = self.X.shape[1]\n",
    "        #obliczenie a priori\n",
    "        self.aPriori = np.array([])\n",
    "        for klasa in np.unique(self.y):\n",
    "            classProb = len(self.y[self.y == klasa])/len(self.y)\n",
    "            self.aPriori = np.append(self.aPriori, classProb)\n",
    "        #obliczenie srednich i odchylen\n",
    "        self.srednia = np.zeros((self.iloscKlas, self.iloscAtrybutow)) #srednia\n",
    "        self.odchylenie = np.zeros((self.iloscKlas, self.iloscAtrybutow)) #odchylenie\n",
    "        itKlasy = 0 \n",
    "        for klasa in np.unique(self.y): # w klasie\n",
    "            itAtryb = 0\n",
    "            for atryb in self.X[self.y==klasa].T: #w tej petli dla kazdego atrybutu inna srednia   # w kolumnie\n",
    "                self.srednia[itKlasy][itAtryb] = np.mean(atryb)\n",
    "                srednia = self.srednia[itKlasy][itAtryb]\n",
    "                self.odchylenie[itKlasy][itAtryb] = np.sqrt(np.sum((atryb-srednia)**2)/(atryb.shape[0]-1))\n",
    "                itAtryb += 1\n",
    "            itKlasy += 1\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        iloscProbek = X.shape[0]\n",
    "        aPosteriori = np.ones((iloscProbek, self.iloscKlas))\n",
    "        probkaIdx = 0\n",
    "        for probka in X:\n",
    "            for klasaIdx in range(self.iloscKlas):\n",
    "                for atrybutIdx in range(self.iloscAtrybutow):\n",
    "                    fWykladnicza = np.exp(-((probka[atrybutIdx]-self.srednia[klasaIdx][atrybutIdx])**2)/(2*self.odchylenie[klasaIdx][atrybutIdx]**2))\n",
    "                    aPosteriori[probkaIdx][klasaIdx] *= 1/(self.odchylenie[klasaIdx][atrybutIdx]*np.sqrt(2*np.pi)) * fWykladnicza\n",
    "                aPosteriori[probkaIdx][klasaIdx] *= self.aPriori[klasaIdx]\n",
    "            probkaIdx += 1\n",
    "        return self.klasy[np.argmax(aPosteriori, axis=1)]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        iloscProbek = X.shape[0]\n",
    "        aPosteriori = np.ones((iloscProbek, self.iloscKlas))\n",
    "        probkaIdx = 0\n",
    "        for probka in X:\n",
    "            for klasaIdx in range(self.iloscKlas):\n",
    "                for atrybutIdx in range(self.iloscAtrybutow):\n",
    "                    fWykladnicza = np.exp(-((probka[atrybutIdx]-self.srednia[klasaIdx][atrybutIdx])**2)/(2*self.odchylenie[klasaIdx][atrybutIdx]**2))\n",
    "                    aPosteriori[probkaIdx][klasaIdx] *= 1/(self.odchylenie[klasaIdx][atrybutIdx]*np.sqrt(2*np.pi)) * fWykladnicza\n",
    "                aPosteriori[probkaIdx][klasaIdx] *= self.aPriori[klasaIdx]\n",
    "            probkaIdx += 1\n",
    "            \n",
    "        prob = np.zeros((aPosteriori.shape[0],aPosteriori.shape[1]))\n",
    "        for i in range(len(prob)):\n",
    "            for j in range(len(prob[0])):\n",
    "                prob[i][j]  = (aPosteriori[i][j]) /np.sum(aPosteriori[i])\n",
    "            \n",
    "        return prob\n",
    "    \n",
    "bcCiagly = BayesClassifierContinuousData()\n",
    "bcCiagly.fit(X_train, y_train)\n",
    "print(\"wariant ciagly\")\n",
    "print(\"dokladnosc na zbiorze testowym: \", bcCiagly.score(X_test, y_test)) \n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "print(\"wariant ciagly - gotowa implementacja\")\n",
    "print(\"dokladnosc na zbiorze testowym: \", gnb.score(X_test,y_test)) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.33, random_state=41)   #zbior testowy\n",
    "bc.fit(X_train, y_train, bins, False)\n",
    "print(\"wariant dyskretny bez poprawki LaPlace\")\n",
    "print(\"dokladnosc na zbiorze testowym: \", bc.score(X_test, y_test))\n",
    "print(\"wariant dyskretny z poprawka LaPlace\")\n",
    "bc.fit(X_train, y_train, bins, True)\n",
    "print(\"dokladnosc na zbiorze testowym: \", bc.score(X_test, y_test))\n",
    "print(\"dokladnosc klasyfikatora Bayesa na danych ciaglych jest wieksza\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przyklad sytuacji niebezpiecznej:\n",
      "wariant ciagly\n",
      "dokladnosc na zbiorze testowym:  0.288135593220339\n",
      "wariant dyskretny\n",
      "dokladnosc na zbiorze testowym:  0.288135593220339\n"
     ]
    }
   ],
   "source": [
    "#Cw 6.4\n",
    "\n",
    "print(\"Przyklad sytuacji niebezpiecznej:\")\n",
    "X = data[:,1:len(data[0])]\n",
    "#powtarzanie kolumn\n",
    "for i in range(7):\n",
    "    X = np.concatenate((X,X),axis=1)\n",
    "#print(X.shape)\n",
    "y = data[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=41)   #zbior testowy\n",
    "bcCiagly = BayesClassifierContinuousData()\n",
    "bcCiagly.fit(X_train, y_train)\n",
    "print(\"wariant ciagly\")\n",
    "print(\"dokladnosc na zbiorze testowym: \", bcCiagly.score(X_test, y_test)) \n",
    "\n",
    "bins = 4\n",
    "est = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='uniform')\n",
    "est.fit(X)\n",
    "Xt = est.transform(X)   #zbior uczacy zdyskretyzowany \n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.33, random_state=41)   #zbior testowy\n",
    "\n",
    "bc.fit(X_train, y_train, bins, True)\n",
    "print(\"wariant dyskretny\")\n",
    "print(\"dokladnosc na zbiorze testowym: \", bc.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wersja bezpieczna\n",
      "wersja dyskretna\n",
      "dokladnosc na zbiorze testowym:  0.9152542372881356\n"
     ]
    }
   ],
   "source": [
    "class BayesClassifierSafe(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y, bins, laPlace = False): #przekazuje fit liczbe przedzialow wartosci\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bins = bins\n",
    "        self.laPlace = laPlace\n",
    "        self.iloscKlas = len(np.unique(y))\n",
    "        self.klasy = np.unique(y)\n",
    "        self.iloscAtrybutow = self.X.shape[1]\n",
    "        #obliczenie a priori\n",
    "        self.aPriori = np.array([])\n",
    "        for klasa in np.unique(self.y):\n",
    "            classProb = len(self.y[self.y == klasa])/len(self.y)\n",
    "            self.aPriori = np.append(self.aPriori, classProb) \n",
    "        #obliczenie rozk warunkowego\n",
    "        self.rozkWarunkowy = np.zeros((self.iloscKlas, self.iloscAtrybutow, self.bins))\n",
    "        itKlasy = 0 \n",
    "        for klasa in np.unique(self.y): # w klasie\n",
    "            itAtryb = 0\n",
    "            for atryb in self.X[self.y==klasa].T: #dla kazdego atrybutu inne prawdopodobienstwo warunkowe   # w kolumnie\n",
    "                for wartosc in range(self.bins): \n",
    "                    if(self.laPlace):\n",
    "                        pWar = (len(atryb[atryb == wartosc]) + 1) / (len(atryb) + self.bins)\n",
    "                    else:\n",
    "                        \n",
    "                        pWar = len(atryb[atryb == wartosc])/len(atryb)\n",
    "                        #bez poprawki LaPlace log(pWar) mogloby przyjac warto -inf \n",
    "                        #print(np.log(pWar))\n",
    "                        #print(len(atryb[atryb == wartosc])/len(atryb))\n",
    "                        #print(len(atryb))\n",
    "                    self.rozkWarunkowy[itKlasy][itAtryb][wartosc] = pWar\n",
    "                itAtryb += 1\n",
    "            itKlasy += 1\n",
    "            \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        iloscProbek = X.shape[0]\n",
    "        aPosteriori = np.zeros((iloscProbek, self.iloscKlas))\n",
    "        probkaIdx = 0\n",
    "        for probka in X:\n",
    "            for klasaIdx in range(self.iloscKlas):\n",
    "                for atrybutIdx in range(self.iloscAtrybutow):\n",
    "                    aPosteriori[probkaIdx][klasaIdx] += np.log(self.rozkWarunkowy[klasaIdx][atrybutIdx][int(probka[atrybutIdx])])\n",
    "                aPosteriori[probkaIdx][klasaIdx] += np.log(self.aPriori[klasaIdx])\n",
    "            probkaIdx += 1\n",
    "        return self.klasy[np.argmax(aPosteriori, axis=1)]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        iloscProbek = X.shape[0]\n",
    "        aPosteriori = np.zeros((iloscProbek, self.iloscKlas))\n",
    "        probkaIdx = 0\n",
    "        for probka in X:\n",
    "            for klasaIdx in range(self.iloscKlas):\n",
    "                for atrybutIdx in range(self.iloscAtrybutow):\n",
    "                    aPosteriori[probkaIdx][klasaIdx] += np.log(self.rozkWarunkowy[klasaIdx][atrybutIdx][int(probka[atrybutIdx])])\n",
    "                aPosteriori[probkaIdx][klasaIdx] += np.log(self.aPriori[klasaIdx])\n",
    "            probkaIdx += 1\n",
    "        \n",
    "        prob = np.zeros((aPosteriori.shape[0],aPosteriori.shape[1]))\n",
    "        for i in range(len(prob)):\n",
    "            for j in range(len(prob[0])):\n",
    "                if(not self.laPlace and (np.sum(aPosteriori[i]) == 0)):   #gdy iloczyn = 0 bo brak poprawki LapPlace, zmieniam prawodpodobienstwa klas na takie same\n",
    "                    prob[i] = 1/self.iloscKlas\n",
    "                    break\n",
    "                else:\n",
    "                    prob[i][j]  = (aPosteriori[i][j]) /np.sum(aPosteriori[i])\n",
    "        return prob\n",
    "\n",
    "bcSafe = BayesClassifierSafe()\n",
    "bcSafe.fit(X_train, y_train, bins, True)\n",
    "print(\"wersja bezpieczna\")\n",
    "print(\"wersja dyskretna\")\n",
    "print(\"dokladnosc na zbiorze testowym: \", bcSafe.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wersja bezpieczna\n",
      "wariant ciagly\n",
      "dokladnosc na zbiorze testowym:  0.9491525423728814\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=41)   #zbior testowy\n",
    "\n",
    "class BayesClassifierContinuousSafe(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y): #przekazuje fit liczbe przedzialow wartosci\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.iloscKlas = len(np.unique(y))\n",
    "        self.klasy = np.unique(y)\n",
    "        self.iloscAtrybutow = self.X.shape[1]\n",
    "        #obliczenie a priori\n",
    "        self.aPriori = np.array([])\n",
    "        for klasa in np.unique(self.y):\n",
    "            classProb = len(self.y[self.y == klasa])/len(self.y)\n",
    "            self.aPriori = np.append(self.aPriori, classProb)\n",
    "        #obliczenie srednich i odchylen\n",
    "        self.srednia = np.zeros((self.iloscKlas, self.iloscAtrybutow)) #srednia\n",
    "        self.odchylenie = np.zeros((self.iloscKlas, self.iloscAtrybutow)) #odchylenie\n",
    "        itKlasy = 0 \n",
    "        for klasa in np.unique(self.y): # w klasie\n",
    "            itAtryb = 0\n",
    "            for atryb in self.X[self.y==klasa].T: #w tej petli dla kazdego atrybutu inna srednia   # w kolumnie\n",
    "                self.srednia[itKlasy][itAtryb] = np.mean(atryb)\n",
    "                srednia = self.srednia[itKlasy][itAtryb]\n",
    "                self.odchylenie[itKlasy][itAtryb] = np.sqrt(np.sum((atryb-srednia)**2)/(atryb.shape[0]-1))\n",
    "                itAtryb += 1\n",
    "            itKlasy += 1\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        iloscProbek = X.shape[0]\n",
    "        aPosteriori = np.zeros((iloscProbek, self.iloscKlas))\n",
    "        probkaIdx = 0\n",
    "        for probka in X:\n",
    "            for klasaIdx in range(self.iloscKlas):\n",
    "                for atrybutIdx in range(self.iloscAtrybutow):\n",
    "                    drogaRoznica = (probka[atrybutIdx]-self.srednia[klasaIdx][atrybutIdx])**2/(2*self.odchylenie[klasaIdx][atrybutIdx]**2)\n",
    "                    aPosteriori[probkaIdx][klasaIdx] += -np.log(self.odchylenie[klasaIdx][atrybutIdx]) - drogaRoznica\n",
    "                aPosteriori[probkaIdx][klasaIdx] += np.log(self.aPriori[klasaIdx])\n",
    "            probkaIdx += 1\n",
    "        return self.klasy[np.argmax(aPosteriori, axis=1)]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        iloscProbek = X.shape[0]\n",
    "        aPosteriori = np.zeros((iloscProbek, self.iloscKlas))\n",
    "        probkaIdx = 0\n",
    "        for probka in X:\n",
    "            for klasaIdx in range(self.iloscKlas):\n",
    "                for atrybutIdx in range(self.iloscAtrybutow):\n",
    "                    drogaRoznica = (probka[atrybutIdx]-self.srednia[klasaIdx][atrybutIdx])**2/(2*self.odchylenie[klasaIdx][atrybutIdx]**2)\n",
    "                    aPosteriori[probkaIdx][klasaIdx] += -np.log(self.odchylenie[klasaIdx][atrybutIdx]) - drogaRoznica\n",
    "                aPosteriori[probkaIdx][klasaIdx] += self.aPriori[klasaIdx]\n",
    "            probkaIdx += 1\n",
    "            \n",
    "        prob = np.zeros((aPosteriori.shape[0],aPosteriori.shape[1]))\n",
    "        for i in range(len(prob)):\n",
    "            for j in range(len(prob[0])):\n",
    "                prob[i][j]  = (aPosteriori[i][j]) /np.sum(aPosteriori[i])\n",
    "                    \n",
    "        return prob\n",
    "\n",
    "    \n",
    "bcCiagly = BayesClassifierContinuousSafe()\n",
    "bcCiagly.fit(X_train, y_train)\n",
    "print(\"wersja bezpieczna\")\n",
    "print(\"wariant ciagly\")\n",
    "print(\"dokladnosc na zbiorze testowym: \", bcCiagly.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wersja dyskretna\n",
      "0.6363636363636364\n",
      "dostepny z neta:  0.6363636363636364\n",
      "0.7272727272727273\n",
      "dostepny z neta:  0.7272727272727273\n",
      "0.7272727272727273\n",
      "dostepny z neta:  0.7272727272727273\n",
      "0.5454545454545454\n",
      "dostepny z neta:  0.5454545454545454\n",
      "0.7272727272727273\n",
      "dostepny z neta:  0.7272727272727273\n",
      "0.8181818181818182\n",
      "dostepny z neta:  0.8181818181818182\n",
      "0.7272727272727273\n",
      "dostepny z neta:  0.7272727272727273\n",
      "0.7272727272727273\n",
      "dostepny z neta:  0.7272727272727273\n",
      "0.7272727272727273\n",
      "dostepny z neta:  0.7272727272727273\n",
      "0.8181818181818182\n",
      "dostepny z neta:  0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "#Cw 6.3\n",
    "#link do zbioru \n",
    "#https://archive.ics.uci.edu/ml/machine-learning-databases/00194/\n",
    "# dataFloat = np.genfromtxt(\"sensor_readings_24.csv\",dtype=float, delimiter=',')\n",
    "# X = dataFloat[:,0:len(dataFloat[0])-1]\n",
    "# print(dataFloat.shape)\n",
    "# dataStr = np.genfromtxt(\"sensor_readings_24.csv\",dtype=str, delimiter=',', usecols=24)\n",
    "# y=dataStr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "print(\"wersja dyskretna\")\n",
    "#bins = 2\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.07)   #zbior testowy\n",
    "    bc = BayesClassifierContinuousSafe()\n",
    "\n",
    "    bc.fit(X_train, y_train)\n",
    "    print(bc.score(X_test, y_test))\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    print(\"dostepny z neta: \", gnb.score(X_test,y_test)) \n",
    "# while(bins < 84):\n",
    "#     #est = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='uniform')\n",
    "#     #est.fit(X)\n",
    "#     #Xt = est.transform(X)   #zbior uczacy zdyskretyzowany \n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.07, random_state=41)   #zbior testowy\n",
    "#     bc = BayesClassifierContinuousSafe()\n",
    "\n",
    "#     bc.fit(X_train, y_train, bins, False)\n",
    "#     print(\"przedzialow: \", bins, end=\"   \")\n",
    "#     print(\"bez LaPlace: \", bc.score(X_test, y_test),end=\" \")\n",
    "#     bc.fit(X_train, y_train, bins, True)\n",
    "#     print(\"LaPlace: \", bc.score(X_test, y_test))\n",
    "#     bins += 5\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
